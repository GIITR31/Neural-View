{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14393606,"sourceType":"datasetVersion","datasetId":9192558}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, json\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport imageio\nfrom tqdm import tqdm\n\n\nDATASET_DIR = \"/kaggle/input/scene\"\nTRANSFORMS_PATH = os.path.join(DATASET_DIR, \"transforms_train.json\")\nMODEL_PATH = \"/kaggle/working/tiny_nerf_01000.pth\" #model\nOUTPUT_VIDEO = \"orbit.mp4\"\n\nH, W = 200, 200\nN_FRAMES = 60\nCHUNK = 1024\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBACKGROUND = \"white\"   # \"white\" or \"black\"\n\n# Tiny-NeRF params\nL_POS = 6\nN_SAMPLES = 64\n\nprint(\"Using device:\", DEVICE)\n\ndef posenc(x, L):\n    out = [x]\n    for i in range(L):\n        out.append(torch.sin((2**i) * np.pi * x))\n        out.append(torch.cos((2**i) * np.pi * x))\n    return torch.cat(out, dim=-1)\n\n\nclass TinyNeRF(nn.Module):\n    def __init__(self, D=8, W=256, input_ch=39, skips=[4]):\n        super().__init__()\n        self.D = D\n        self.W = W\n        self.input_ch = input_ch\n        self.skips = skips\n\n        self.pts_linears = nn.ModuleList(\n            [nn.Linear(input_ch, W)] +\n            [\n                nn.Linear(W + input_ch, W) if i in skips\n                else nn.Linear(W, W)\n                for i in range(1, D)\n            ]\n        )\n\n        self.output_linear = nn.Linear(W, 4)  # RGB + sigma\n\n    def forward(self, x):\n        h = x\n        for i, l in enumerate(self.pts_linears):\n            if i in self.skips:\n                h = torch.cat([x, h], dim=-1)  # 256 + 39 = 295\n            h = F.relu(l(h))\n\n        out = self.output_linear(h)\n        rgb = torch.sigmoid(out[..., :3])\n        sigma = F.relu(out[..., 3])\n        return rgb, sigma\n\ndef get_rays(H, W, focal, c2w):\n    i, j = torch.meshgrid(\n        torch.arange(W, device=DEVICE),\n        torch.arange(H, device=DEVICE),\n        indexing=\"xy\"\n    )\n    dirs = torch.stack([\n        (i - W * 0.5) / focal,\n        -(j - H * 0.5) / focal,\n        -torch.ones_like(i)\n    ], -1)\n\n    rays_d = (dirs[..., None, :] * c2w[:3, :3]).sum(-1)\n    rays_o = c2w[:3, 3].expand_as(rays_d)\n    return rays_o, rays_d\n\n\ndef render_rays(model, rays_o, rays_d, near, far):\n    z_vals = torch.linspace(near, far, N_SAMPLES, device=DEVICE)\n    z_vals = z_vals.expand(rays_o.shape[0], N_SAMPLES)\n\n    # Sample points along rays\n    pts = rays_o[:, None, :] + rays_d[:, None, :] * z_vals[..., None]\n\n    # Positional encoding (Tiny-NeRF → ONLY xyz)\n    pts_enc = posenc(pts.reshape(-1, 3), L_POS)\n\n    # Forward pass\n    rgb, sigma = model(pts_enc)\n\n    rgb = rgb.view(-1, N_SAMPLES, 3)\n    sigma = sigma.view(-1, N_SAMPLES)\n\n    # Volume rendering\n    dists = z_vals[:, 1:] - z_vals[:, :-1]\n    dists = torch.cat(\n        [dists, 1e10 * torch.ones_like(dists[:, :1])],\n        dim=-1\n    )\n\n    alpha = 1.0 - torch.exp(-sigma * dists)\n\n    T = torch.cumprod(\n        torch.cat(\n            [torch.ones_like(alpha[:, :1]), 1.0 - alpha + 1e-10],\n            dim=-1\n        ),\n        dim=-1\n    )[:, :-1]\n\n    weights = alpha * T\n\n    rgb_map = torch.sum(weights[..., None] * rgb, dim=1)\n\n    if BACKGROUND == \"white\":\n        rgb_map = rgb_map + (1.0 - weights.sum(dim=1, keepdim=True))\n\n    return rgb_map\n\n\n@torch.no_grad()\ndef render_image(model, rays_o, rays_d, near, far):\n    H_, W_ = rays_o.shape[:2]\n    rays_o = rays_o.reshape(-1, 3)\n    rays_d = rays_d.reshape(-1, 3)\n\n    out = []\n    for i in range(0, rays_o.shape[0], CHUNK):\n        rgb = render_rays(\n            model,\n            rays_o[i:i+CHUNK],\n            rays_d[i:i+CHUNK],\n            near, far\n        )\n        out.append(rgb.cpu())\n\n    return torch.cat(out).reshape(H_, W_, 3)\n\n\n\n\ndef pose_spherical(theta_deg, radius, device):\n    theta = np.deg2rad(theta_deg)\n\n    cam_pos = torch.tensor(\n        [radius * np.sin(theta), 0.0, radius * np.cos(theta)],\n        device=device,\n        dtype=torch.float32\n    )\n\n    target = torch.tensor([0.0, 0.0, 0.0], device=device, dtype=torch.float32)\n    up = torch.tensor([0.0, 1.0, 0.0], device=device, dtype=torch.float32)\n\n    forward = target - cam_pos\n    forward = forward / torch.norm(forward)\n\n    right = torch.cross(forward, up)\n    right = right / torch.norm(right)\n\n    up = torch.cross(right, forward)\n\n    c2w = torch.eye(4, device=device, dtype=torch.float32)\n    c2w[:3, 0] = right\n    c2w[:3, 1] = up\n    c2w[:3, 2] = -forward\n    c2w[:3, 3] = cam_pos\n\n    return c2w\n   \n\ndef main():\n    with open(TRANSFORMS_PATH) as f:\n        meta = json.load(f)\n\n    focal = 0.5 * W / np.tan(0.5 * meta[\"camera_angle_x\"])\n\n    model = TinyNeRF().to(DEVICE)\n    ckpt = torch.load(MODEL_PATH, map_location=DEVICE)\n    model.load_state_dict(ckpt[\"model\"])\n    model.eval()\n\n    near = 0.8 * min([np.linalg.norm(f[\"transform_matrix\"][0][3]) for f in meta[\"frames\"]])\n    far  = 1.2 * max([np.linalg.norm(f[\"transform_matrix\"][0][3]) for f in meta[\"frames\"]])\n\n    frames = []\n    for th in tqdm(np.linspace(0, 360, N_FRAMES, endpoint=False)):\n        c2w = pose_spherical(th, radius=4.0,device= DEVICE)\n        rays_o, rays_d = get_rays(H, W, focal, c2w)\n        img = render_image(model, rays_o, rays_d, near, far)\n        frames.append((img.numpy().clip(0, 1) * 255).astype(np.uint8))\n\n    imageio.mimwrite(OUTPUT_VIDEO, frames, fps=12)\n    print(\"Saved:\", OUTPUT_VIDEO)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:51:08.311569Z","iopub.execute_input":"2026-01-05T15:51:08.312157Z","iopub.status.idle":"2026-01-05T15:52:13.619780Z","shell.execute_reply.started":"2026-01-05T15:51:08.312126Z","shell.execute_reply":"2026-01-05T15:52:13.618795Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 60/60 [01:05<00:00,  1.09s/it]\nIMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (200, 200) to (208, 208) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","output_type":"stream"},{"name":"stdout","text":"Saved: orbit.mp4\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"!pip install mcubes\n!pip install trimesh","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:58:45.329080Z","iopub.execute_input":"2026-01-05T15:58:45.329784Z","iopub.status.idle":"2026-01-05T15:58:56.660115Z","shell.execute_reply.started":"2026-01-05T15:58:45.329749Z","shell.execute_reply":"2026-01-05T15:58:56.659403Z"}},"outputs":[{"name":"stdout","text":"Collecting mcubes\n  Downloading mcubes-0.1.6-cp310-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\nDownloading mcubes-0.1.6-cp310-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (293 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.3/293.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: mcubes\nSuccessfully installed mcubes-0.1.6\nCollecting trimesh\n  Downloading trimesh-4.10.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.12/dist-packages (from trimesh) (2.0.2)\nDownloading trimesh-4.10.1-py3-none-any.whl (737 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m737.0/737.0 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: trimesh\nSuccessfully installed trimesh-4.10.1\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"\nimport mcubes\nfrom skimage import measure\nimport trimesh\nfrom scipy.ndimage import gaussian_filter\n\nOUTPUT_MESH = \"nerf_mesh.ply\"\n\nGRID_RES = 200        \nSIGMA_THRESHOLD = 8.0\nCHUNK = 65536\n\nprint(\"Generating density grid for mesh...\")\n\n\nlin = torch.linspace(-0.6, 0.6, GRID_RES, device=DEVICE)\nX, Y, Z = torch.meshgrid(lin, lin, lin, indexing=\"ij\")\npts = torch.stack([X, Y, Z], dim=-1).reshape(-1, 3)\n\n# Tiny / Full NeRF does NOT use view direction for density\nviews = torch.zeros_like(pts)\n\n\nsigmas = []\n\nwith torch.no_grad():\n    for i in tqdm(range(0, pts.shape[0], CHUNK)):\n        p = pts[i:i+CHUNK]\n        v = views[i:i+CHUNK]\n\n        p_enc = posenc(p, L_POS)\n\n        # Full NeRF expects view encoding → keep zero\n        if \"L_DIR\" in globals():\n            v_enc = posenc(v, L_DIR)\n            _, sigma = model(torch.cat([p_enc, v_enc], dim=-1))\n        else:\n            _, sigma = model(p_enc)\n\n        sigmas.append(sigma.squeeze(-1).cpu())\n\nsigma_grid = torch.cat(sigmas).reshape(GRID_RES, GRID_RES, GRID_RES).numpy()\n\nprint(\"Density grid computed\")\n\n\nsigma_grid = gaussian_filter(sigma_grid, sigma=0.5)\n\n\nverts, faces, normals, values = measure.marching_cubes(\n    sigma_grid,\n    level=SIGMA_THRESHOLD\n)\n\nverts = verts / (GRID_RES - 1) * 2.4 - 1.2\n\nmesh = trimesh.Trimesh(\n    vertices=verts,\n    faces=faces,\n    vertex_normals=normals,\n    process=False\n)\n\n\n\nmesh.export(OUTPUT_MESH)\n\nprint(f\"Mesh saved to {OUTPUT_MESH}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}