{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "DATASET_PATH = \"/kaggle/input/scene/\"\n",
    "WHITE_BG = True   # True = white background, False = black background\n",
    "\n",
    "VAL_IMG_DIR = \"val_images\"\n",
    "\n",
    "H_RES, W_RES = 800, 800\n",
    "\n",
    "L_POS = 6                  # Tiny-NeRF uses L=6\n",
    "N_ITERS = 50000\n",
    "BATCH_SIZE = 2048\n",
    "LR = 5e-4\n",
    "\n",
    "N_SAMPLES = 64              # single sampling only\n",
    "VAL_INTERVAL = 100\n",
    "SAVE_INTERVAL = 1000\n",
    "\n",
    "DEVICE = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "\n",
    "os.makedirs(VAL_IMG_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "def posenc(x, L):\n",
    "    out = [x]\n",
    "    for i in range(L):\n",
    "        out.append(torch.sin((2**i) * x))\n",
    "        out.append(torch.cos((2**i) * x))\n",
    "    return torch.cat(out, -1)\n",
    "\n",
    "\n",
    "\n",
    "def load_data(basedir, H, W):\n",
    "    with open(os.path.join(basedir, \"transforms_train.json\")) as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    images, poses = [], []\n",
    "\n",
    "    for frame in meta[\"frames\"]:\n",
    "        img_path = os.path.join(basedir, frame[\"file_path\"])\n",
    "        if not os.path.exists(img_path):\n",
    "            continue\n",
    "\n",
    "        img = Image.open(img_path).resize((W, H), Image.LANCZOS)\n",
    "        img = np.array(img).astype(np.float32) / 255.0\n",
    "\n",
    "        if img.shape[-1] == 4:\n",
    "            alpha = img[..., 3:4]\n",
    "            if WHITE_BG:\n",
    "                img = img[..., :3] * alpha + (1 - alpha)\n",
    "            else:\n",
    "                img = img[..., :3] * alpha\n",
    "\n",
    "        images.append(img)\n",
    "        poses.append(np.array(frame[\"transform_matrix\"], dtype=np.float32))\n",
    "\n",
    "    images = torch.tensor(np.array(images), device=DEVICE)\n",
    "    poses = torch.tensor(np.array(poses), device=DEVICE)\n",
    "\n",
    "    poses[:, :3, 1] *= -1\n",
    "    poses[:, :3, 2] *= -1\n",
    "\n",
    "    # --- Center + scale (LLFF) ---\n",
    "    centers = poses[:, :3, 3]\n",
    "    center = centers.mean(0)\n",
    "    poses[:, :3, 3] -= center\n",
    "    scale = 2.0 / torch.norm(poses[:, :3, 3], dim=1).mean()\n",
    "    poses[:, :3, 3] *= scale\n",
    "\n",
    "    focal = 0.5 * W / np.tan(0.5 * meta[\"camera_angle_x\"])\n",
    "\n",
    "    return images, poses, focal\n",
    "\n",
    "\n",
    "def get_rays(H, W, focal, c2w):\n",
    "    i, j = torch.meshgrid(\n",
    "        torch.arange(W, device=DEVICE),\n",
    "        torch.arange(H, device=DEVICE),\n",
    "        indexing=\"xy\"\n",
    "    )\n",
    "    dirs = torch.stack([\n",
    "        (i - W * 0.5) / focal,\n",
    "        -(j - H * 0.5) / focal,\n",
    "        -torch.ones_like(i)\n",
    "    ], -1)\n",
    "\n",
    "    rays_d = (dirs[..., None, :] * c2w[:3, :3]).sum(-1)\n",
    "    rays_o = c2w[:3, 3].expand_as(rays_d)\n",
    "    return rays_o, rays_d\n",
    "\n",
    "\n",
    "def raw2outputs(rgb, sigma, z_vals, rays_d, white_bg=True):\n",
    "    dists = z_vals[..., 1:] - z_vals[..., :-1]\n",
    "    dists = torch.cat(\n",
    "        [dists, 1e10 * torch.ones_like(dists[..., :1])], -1\n",
    "    )\n",
    "    dists *= torch.norm(rays_d[..., None, :], dim=-1)\n",
    "\n",
    "    alpha = 1.0 - torch.exp(-sigma * dists)\n",
    "\n",
    "    T = torch.cumprod(\n",
    "        torch.cat(\n",
    "            [torch.ones_like(alpha[..., :1]),\n",
    "             1. - alpha + 1e-10],\n",
    "            -1\n",
    "        ),\n",
    "        -1\n",
    "    )[..., :-1]\n",
    "\n",
    "    weights = alpha * T\n",
    "    rgb_map = torch.sum(weights[..., None] * rgb, -2)\n",
    "\n",
    "    acc = torch.sum(weights, -1)\n",
    "\n",
    "    if white_bg:\n",
    "        rgb_map = rgb_map + (1.0 - acc)[..., None]\n",
    "\n",
    "    return rgb_map\n",
    "\n",
    "class TinyNeRF(nn.Module):\n",
    "    def __init__(self, D=8, W=256, input_ch=39, skips=[4]):\n",
    "        super().__init__()\n",
    "        self.D = D\n",
    "        self.W = W\n",
    "        self.input_ch = input_ch\n",
    "        self.skips = skips\n",
    "\n",
    "        self.pts_linears = nn.ModuleList(\n",
    "            [nn.Linear(input_ch, W)] +\n",
    "            [\n",
    "                nn.Linear(W + input_ch, W) if i in skips\n",
    "                else nn.Linear(W, W)\n",
    "                for i in range(1, D)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.output_linear = nn.Linear(W, 4)  # RGB + sigma\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        for i, l in enumerate(self.pts_linears):\n",
    "            if i in self.skips:\n",
    "                h = torch.cat([x, h], dim=-1)  # 256 + 39 = 295\n",
    "            h = F.relu(l(h))\n",
    "\n",
    "        out = self.output_linear(h)\n",
    "        rgb = torch.sigmoid(out[..., :3])\n",
    "        sigma = F.relu(out[..., 3])\n",
    "        return rgb, sigma\n",
    "\n",
    "\n",
    "def render_rays(rays_o, rays_d, model, near, far):\n",
    "    N = rays_o.shape[0]\n",
    "\n",
    "    z_vals = torch.linspace(near, far, N_SAMPLES, device=DEVICE)\n",
    "    z_vals = z_vals.expand(N, N_SAMPLES)\n",
    "\n",
    "    pts = rays_o[:, None] + rays_d[:, None] * z_vals[..., None]\n",
    "    pts_enc = posenc(pts.reshape(-1, 3), L_POS)\n",
    "\n",
    "    rgb, sigma = model(pts_enc)\n",
    "    rgb = rgb.view(N, N_SAMPLES, 3)\n",
    "    sigma = sigma.view(N, N_SAMPLES)\n",
    "\n",
    "    return raw2outputs(rgb, sigma, z_vals, rays_d,white_bg=WHITE_BG)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def render_full_image(H, W, focal, pose, model, near, far, chunk=2048):\n",
    "    rays_o, rays_d = get_rays(H, W, focal, pose)\n",
    "    rays_o = rays_o.reshape(-1, 3)\n",
    "    rays_d = rays_d.reshape(-1, 3)\n",
    "\n",
    "    outputs = []\n",
    "    for i in range(0, rays_o.shape[0], chunk):\n",
    "        outputs.append(\n",
    "            render_rays(\n",
    "                rays_o[i:i+chunk],\n",
    "                rays_d[i:i+chunk],\n",
    "                model, near, far\n",
    "            ).cpu()\n",
    "        )\n",
    "\n",
    "    return torch.cat(outputs).reshape(H, W, 3)\n",
    "\n",
    "\n",
    "def save_checkpoint(i, model, optimizer):\n",
    "    torch.save({\n",
    "        \"iter\": i,\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict()\n",
    "    }, f\"tiny_nerf_{i:05d}.pth\")\n",
    "\n",
    "\n",
    "images, poses, focal = load_data(DATASET_PATH, H_RES, W_RES)\n",
    "\n",
    "cam_dists = torch.norm(poses[:, :3, 3], dim=1)\n",
    "near = cam_dists.min().item()\n",
    "far = cam_dists.max().item()\n",
    "\n",
    "val_idx = torch.argmin(cam_dists)\n",
    "val_img = images[val_idx]\n",
    "val_pose = poses[val_idx]\n",
    "\n",
    "model = TinyNeRF().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "for i in range(1, N_ITERS + 1):\n",
    "    img_i = np.random.randint(images.shape[0])\n",
    "\n",
    "    rays_o, rays_d = get_rays(H_RES, W_RES, focal, poses[img_i])\n",
    "    coords = torch.randint(0, H_RES, (BATCH_SIZE, 2), device=DEVICE)\n",
    "\n",
    "    rgb = render_rays(\n",
    "        rays_o[coords[:, 0], coords[:, 1]],\n",
    "        rays_d[coords[:, 0], coords[:, 1]],\n",
    "        model, near, far\n",
    "    )\n",
    "\n",
    "    gt = images[img_i][coords[:, 0], coords[:, 1]]\n",
    "    loss = F.mse_loss(rgb, gt)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        psnr = -10. * torch.log10(loss)\n",
    "        print(f\"Iter {i:05d} | PSNR {psnr.item():.2f}\")\n",
    "\n",
    "    if i % VAL_INTERVAL == 0:\n",
    "        rgb_val = render_full_image(\n",
    "            H_RES, W_RES, focal, val_pose, model, near, far\n",
    "        )\n",
    "\n",
    "        rgb_val = rgb_val.to(\"cuda\")\n",
    "        val_img = val_img.to(\"cuda\")\n",
    "        \n",
    "\n",
    "        val_psnr = -10. * torch.log10(F.mse_loss(rgb_val, val_img))\n",
    "        print(f\"[VAL] Iter {i:05d} | PSNR {val_psnr.item():.2f}\")\n",
    "\n",
    "        plt.imsave(\n",
    "            f\"{VAL_IMG_DIR}/val_{i:05d}.png\",\n",
    "            rgb_val.clamp(0,1).cpu().numpy()\n",
    "        )\n",
    "\n",
    "    if i % SAVE_INTERVAL == 0:\n",
    "        save_checkpoint(i, model, optimizer)\n",
    "\n",
    "print(\"Tiny-NeRF training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9192558,
     "sourceId": 14393606,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
