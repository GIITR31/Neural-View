{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0475a422-5bb8-4906-9175-a6508c51cb14",
    "_uuid": "bb8b4cb7-3d4f-423b-beb9-2230bb86d30d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-05T11:31:32.519982Z",
     "iopub.status.busy": "2026-01-05T11:31:32.519659Z",
     "iopub.status.idle": "2026-01-05T11:36:09.654850Z",
     "shell.execute_reply": "2026-01-05T11:36:09.653898Z",
     "shell.execute_reply.started": "2026-01-05T11:31:32.519955Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os, json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "DATASET_PATH = \"/kaggle/input/scene\"            # folder with images/ + transforms_train.json\n",
    "\n",
    "VAL_IMG_DIR = \"val_images\"\n",
    "\n",
    "H_RES, W_RES = 800, 800\n",
    "L_POS, L_DIR = 10, 4\n",
    "USE_WHITE_BG = True\n",
    "\n",
    "N_ITERS = 50000\n",
    "BATCH_SIZE = 2048\n",
    "LR = 5e-4\n",
    "\n",
    "N_COARSE = 64\n",
    "N_FINE = 128\n",
    "\n",
    "VAL_INTERVAL = 200\n",
    "SAVE_INTERVAL = 1000\n",
    "NOISE_DECAY_ITERS = 2000\n",
    "\n",
    "DEVICE = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(\"Using device:\", DEVICE)\n",
    "print(\"GPU count:\", torch.cuda.device_count())\n",
    "\n",
    "\n",
    "os.makedirs(VAL_IMG_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "def posenc(x, L):\n",
    "    out = [x]\n",
    "    for i in range(L):\n",
    "        out.append(torch.sin((2**i) * np.pi * x))\n",
    "        out.append(torch.cos((2**i) * np.pi * x))\n",
    "    return torch.cat(out, -1)\n",
    "\n",
    "\n",
    "\n",
    "def load_data(basedir, H, W):\n",
    "    with open(os.path.join(basedir, \"transforms_train.json\")) as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    images, poses = [], []\n",
    "\n",
    "    for frame in meta[\"frames\"]:\n",
    "        fp = frame[\"file_path\"].replace(\"./\", \"\")\n",
    "        # if not fp.endswith(\".png\"):\n",
    "        #     fp += \".png\"\n",
    "\n",
    "        img_path = os.path.join(basedir, fp)\n",
    "        print(img_path)\n",
    "        if not os.path.exists(img_path):\n",
    "            continue\n",
    "\n",
    "        img = Image.open(img_path).resize((W, H), Image.LANCZOS)\n",
    "        img = np.array(img).astype(np.float32) / 255.0\n",
    "\n",
    "        if img.shape[-1] == 4:\n",
    "            alpha = img[..., 3:4]\n",
    "            if USE_WHITE_BG:\n",
    "                img = img[..., :3] * alpha + (1 - alpha)\n",
    "            else:\n",
    "                img = img[..., :3] * alpha\n",
    "                \n",
    "        images.append(img)\n",
    "        poses.append(np.array(frame[\"transform_matrix\"], dtype=np.float32))\n",
    "\n",
    "    images = torch.tensor(np.array(images), device=DEVICE)\n",
    "    poses = torch.tensor(np.array(poses), device=DEVICE)\n",
    "\n",
    "    print(\"poses shape:\", poses.shape)\n",
    "\n",
    "\n",
    "    poses[:, :3, 1] *= -1   # flip Y axis\n",
    "    poses[:, :3, 2] *= -1   # flip Z axis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Center + scale (LLFF style)\n",
    "\n",
    "    centers = poses[:, :3, 3]\n",
    "    center = centers.mean(0)\n",
    "    poses[:, :3, 3] -= center\n",
    "    scale = 2.0 / torch.norm(poses[:, :3, 3], dim=1).mean()\n",
    "    poses[:, :3, 3] *= scale\n",
    "\n",
    "    focal = 0.5 * W / np.tan(0.5 * meta[\"camera_angle_x\"])\n",
    "\n",
    "    return images, poses, focal\n",
    "\n",
    "def get_rays(H, W, focal, c2w):\n",
    "    i, j = torch.meshgrid(\n",
    "        torch.arange(W, device=DEVICE),\n",
    "        torch.arange(H, device=DEVICE),\n",
    "        indexing=\"xy\"\n",
    "    )\n",
    "    dirs = torch.stack([\n",
    "        (i - W * 0.5) / focal,\n",
    "        -(j - H * 0.5) / focal,\n",
    "        -torch.ones_like(i)\n",
    "    ], -1)\n",
    "\n",
    "    rays_d = (dirs[..., None, :] * c2w[:3, :3]).sum(-1)\n",
    "    rays_o = c2w[:3, 3].expand_as(rays_d)\n",
    "    return rays_o, rays_d\n",
    "\n",
    "\n",
    "def sample_pdf(bins, weights, N):\n",
    "    weights = weights + 1e-5\n",
    "    pdf = weights / torch.sum(weights, -1, keepdim=True)\n",
    "    cdf = torch.cumsum(pdf, -1)\n",
    "    cdf = torch.cat([torch.zeros_like(cdf[..., :1]), cdf], -1)\n",
    "\n",
    "    u = torch.rand(list(cdf.shape[:-1]) + [N], device=DEVICE)\n",
    "    inds = torch.searchsorted(cdf, u, right=True)\n",
    "\n",
    "    below = torch.clamp(inds - 1, min=0)\n",
    "    above = torch.clamp(inds, max=cdf.shape[-1] - 1)\n",
    "    inds = torch.stack([below, above], -1)\n",
    "\n",
    "    cdf_g = torch.gather(cdf.unsqueeze(-2).expand(*inds.shape[:-1], cdf.shape[-1]), -1, inds)\n",
    "    bins_g = torch.gather(bins.unsqueeze(-2).expand(*inds.shape[:-1], bins.shape[-1]), -1, inds)\n",
    "\n",
    "    t = (u - cdf_g[..., 0]) / (cdf_g[..., 1] - cdf_g[..., 0] + 1e-5)\n",
    "    return bins_g[..., 0] + t * (bins_g[..., 1] - bins_g[..., 0])\n",
    "\n",
    "def raw2outputs(raw, z_vals, rays_d, noise_std):\n",
    "    dists = z_vals[..., 1:] - z_vals[..., :-1]\n",
    "    dists = torch.cat([dists, 1e10 * torch.ones_like(dists[..., :1])], -1)\n",
    "    dists *= torch.norm(rays_d[..., None, :], dim=-1)\n",
    "\n",
    "    rgb = torch.sigmoid(raw[..., :3])\n",
    "    noise = torch.randn_like(raw[..., 3]) * noise_std if noise_std > 0 else 0\n",
    "    sigma = F.softplus(raw[..., 3] + noise)\n",
    "\n",
    "    alpha = 1. - torch.exp(-sigma * dists)\n",
    "    T = torch.cumprod(\n",
    "        torch.cat([torch.ones_like(alpha[..., :1]), 1. - alpha + 1e-10], -1),\n",
    "        -1\n",
    "    )[..., :-1]\n",
    "\n",
    "    weights = alpha * T\n",
    "    rgb_map = torch.sum(weights[..., None] * rgb, -2)\n",
    "\n",
    "    if USE_WHITE_BG:\n",
    "        acc = torch.sum(weights, -1, keepdim=True)\n",
    "        rgb_map = rgb_map + (1 - acc)\n",
    "\n",
    "    return rgb_map, weights\n",
    "\n",
    "\n",
    "class NeRF(nn.Module):\n",
    "    def __init__(self, W=256):\n",
    "        super().__init__()\n",
    "        self.fc = nn.ModuleList(\n",
    "            [nn.Linear(63, W)] + [nn.Linear(W, W) for _ in range(7)]\n",
    "        )\n",
    "        self.sigma = nn.Linear(W, 1)\n",
    "        self.rgb = nn.Sequential(\n",
    "            nn.Linear(W + 27, W),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(W, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        pts, views = torch.split(x, [63, 27], -1)\n",
    "        h = pts\n",
    "        for l in self.fc:\n",
    "            h = F.relu(l(h))\n",
    "        sigma = self.sigma(h)\n",
    "        rgb = self.rgb(torch.cat([h, views], -1))\n",
    "        return torch.cat([rgb, sigma], -1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def render_rays(rays_o, rays_d, model_c, model_f, near, far, noise_std):\n",
    "    N = rays_o.shape[0]\n",
    "\n",
    "    z_vals = torch.linspace(near, far, N_COARSE, device=DEVICE).expand(N, N_COARSE)\n",
    "    mids = 0.5 * (z_vals[:, 1:] + z_vals[:, :-1])\n",
    "    z_vals = torch.cat([\n",
    "        z_vals[:, :1],\n",
    "        mids + torch.rand_like(mids) * (z_vals[:, 1:] - z_vals[:, :-1]),\n",
    "        z_vals[:, -1:]\n",
    "    ], -1)\n",
    "\n",
    "    pts = rays_o[:, None] + rays_d[:, None] * z_vals[..., None]\n",
    "    viewdirs = rays_d / torch.norm(rays_d, dim=-1, keepdim=True)\n",
    "    viewdirs = viewdirs[:, None].expand_as(pts)\n",
    "\n",
    "    raw_c = model_c(torch.cat([\n",
    "        posenc(pts.reshape(-1, 3), L_POS),\n",
    "        posenc(viewdirs.reshape(-1, 3), L_DIR)\n",
    "    ], -1)).reshape(N, -1, 4)\n",
    "\n",
    "    rgb_c, weights = raw2outputs(raw_c, z_vals, rays_d, noise_std)\n",
    "\n",
    "    z_mid = 0.5 * (z_vals[:, 1:] + z_vals[:, :-1])\n",
    "    z_fine = sample_pdf(z_mid, weights[:, 1:-1], N_FINE)\n",
    "    z_vals = torch.sort(torch.cat([z_vals, z_fine], -1), -1)[0]\n",
    "\n",
    "    pts = rays_o[:, None] + rays_d[:, None] * z_vals[..., None]\n",
    "    viewdirs = viewdirs[:, :1].expand_as(pts)\n",
    "\n",
    "    raw_f = model_f(torch.cat([\n",
    "        posenc(pts.reshape(-1, 3), L_POS),\n",
    "        posenc(viewdirs.reshape(-1, 3), L_DIR)\n",
    "    ], -1)).reshape(N, -1, 4)\n",
    "\n",
    "    rgb_f, _ = raw2outputs(raw_f, z_vals, rays_d, noise_std)\n",
    "    return rgb_f\n",
    "\n",
    "@torch.no_grad()\n",
    "def render_full_image(H, W, focal, pose, model_c, model_f, near, far):\n",
    "    rays_o, rays_d = get_rays(H, W, focal, pose)\n",
    "    rays_o, rays_d = rays_o.reshape(-1, 3), rays_d.reshape(-1, 3)\n",
    "\n",
    "    outputs = []\n",
    "    for i in range(0, rays_o.shape[0], 4096):\n",
    "        outputs.append(\n",
    "            render_rays(\n",
    "                rays_o[i:i+4096],\n",
    "                rays_d[i:i+4096],\n",
    "                model_c, model_f,\n",
    "                near, far, 0.0\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return torch.cat(outputs).reshape(H, W, 3)\n",
    "\n",
    "def save_checkpoint(iteration, model_c, model_f, optimizer):\n",
    "    ckpt = {\n",
    "        \"iter\": iteration,\n",
    "        \"model_c\": model_c.module.state_dict() if isinstance(model_c, nn.DataParallel) else model_c.state_dict(),\n",
    "        \"model_f\": model_f.module.state_dict() if isinstance(model_f, nn.DataParallel) else model_f.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(ckpt, f\"ckpt_{iteration:05d}.pth\")\n",
    "\n",
    "\n",
    "images, poses, focal = load_data(DATASET_PATH, H_RES, W_RES)\n",
    "\n",
    "cam_dists = torch.norm(poses[:, :3, 3], dim=1)\n",
    "val_idx = torch.argmin(cam_dists)\n",
    "\n",
    "val_img = images[val_idx]\n",
    "val_pose = poses[val_idx]\n",
    "\n",
    "near = cam_dists.min().item() * 0.8\n",
    "far = cam_dists.max().item() * 1.2\n",
    "\n",
    "print(near)\n",
    "print(far)\n",
    "\n",
    "model_c = NeRF().to(DEVICE)\n",
    "model_f = NeRF().to(DEVICE)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model_c = nn.DataParallel(model_c)\n",
    "    model_f = nn.DataParallel(model_f)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(model_c.parameters()) + list(model_f.parameters()), lr=LR\n",
    ")\n",
    "\n",
    "for i in range(1, N_ITERS + 1):\n",
    "    noise_std = max(0.0, 1.0 - i / NOISE_DECAY_ITERS)\n",
    "\n",
    "    img_i = np.random.randint(images.shape[0])\n",
    "    rays_o, rays_d = get_rays(H_RES, W_RES, focal, poses[img_i])\n",
    "    coords = torch.randint(0, H_RES, (BATCH_SIZE, 2), device=DEVICE)\n",
    "\n",
    "    rgb = render_rays(\n",
    "        rays_o[coords[:, 0], coords[:, 1]],\n",
    "        rays_d[coords[:, 0], coords[:, 1]],\n",
    "        model_c, model_f,\n",
    "        near, far, noise_std\n",
    "    )\n",
    "\n",
    "    gt = images[img_i][coords[:, 0], coords[:, 1]]\n",
    "    loss = F.mse_loss(rgb, gt)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        psnr = -10 * torch.log10(loss)\n",
    "        print(f\"Iter {i:05d} | PSNR {psnr.item():.2f}\")\n",
    "\n",
    "    if i % VAL_INTERVAL == 0:\n",
    "        rgb_val = render_full_image(\n",
    "            H_RES, W_RES, focal, val_pose, model_c, model_f, near, far\n",
    "        )\n",
    "        val_psnr = -10 * torch.log10(F.mse_loss(rgb_val, val_img))\n",
    "        print(f\"[VAL] Iter {i:05d} | PSNR {val_psnr.item():.2f}\")\n",
    "\n",
    "        plt.imsave(\n",
    "            f\"{VAL_IMG_DIR}/val_{i:05d}.png\",\n",
    "            rgb_val.clamp(0, 1).cpu().numpy()\n",
    "        )\n",
    "\n",
    "    if i % SAVE_INTERVAL == 0:\n",
    "        save_checkpoint(\n",
    "            iteration=i,\n",
    "            model_c=model_c,\n",
    "            model_f=model_f,\n",
    "            optimizer=optimizer,\n",
    "        )\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
