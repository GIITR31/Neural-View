{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14393866,"sourceType":"datasetVersion","datasetId":9192728},{"sourceId":709049,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":538527,"modelId":551810},{"sourceId":709053,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":538530,"modelId":551812}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport imageio\nfrom tqdm import tqdm\n\nDATASET_DIR = \"/kaggle/input/scene/\"  # ------> your path of the dataset\nTRANSFORMS_PATH = os.path.join(DATASET_DIR, \"transforms_train.json\")\nMODEL_PATH = \"/kaggle/input/model\"  # ------> your path of the model\nOUTPUT_VIDEO = \"orbit.mp4\"\n\nH, W = 200, 200          # KEEP LOW for safety\nN_FRAMES = 60\nCHUNK = 1024             # controls memory\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Background: \"black\" or \"white\"\nBACKGROUND = \"white\"\n\n\nL_POS = 10\nL_DIR = 4\nN_SAMPLES = 64            # SINGLE sampling → stable & memory-safe\nNEAR = 2.0\nFAR = 6.0\n\n\ndef posenc(x, L):\n    out = [x]\n    for i in range(L):\n        out.append(torch.sin((2 ** i) * np.pi * x))\n        out.append(torch.cos((2 ** i) * np.pi * x))\n    return torch.cat(out, -1)\n\n\nclass NeRF(nn.Module):\n    def __init__(self, W=256):\n        super().__init__()\n        self.fc = nn.ModuleList(\n            [nn.Linear(63, W)] + [nn.Linear(W, W) for _ in range(7)]\n        )\n        self.sigma = nn.Linear(W, 1)\n        self.rgb = nn.Sequential(\n            nn.Linear(W + 27, W),\n            nn.ReLU(),\n            nn.Linear(W, 3)\n        )\n\n    def forward(self, x):\n        pts, views = torch.split(x, [63, 27], -1)\n        h = pts\n        for l in self.fc:\n            h = F.relu(l(h))\n        sigma = F.relu(self.sigma(h))\n        rgb = torch.sigmoid(self.rgb(torch.cat([h, views], -1)))\n        return rgb, sigma\n\n\ndef get_rays(H, W, focal, c2w):\n    i, j = torch.meshgrid(\n        torch.arange(W, device=DEVICE),\n        torch.arange(H, device=DEVICE),\n        indexing=\"xy\"\n    )\n    dirs = torch.stack([\n        (i - W * 0.5) / focal,\n        -(j - H * 0.5) / focal,\n        -torch.ones_like(i)\n    ], -1)\n\n    rays_d = (dirs[..., None, :] * c2w[:3, :3]).sum(-1)\n    rays_o = c2w[:3, 3].expand_as(rays_d)\n    return rays_o, rays_d\n\n\ndef render_rays(model, rays_o, rays_d):\n    z_vals = torch.linspace(NEAR, FAR, N_SAMPLES, device=DEVICE)\n    z_vals = z_vals.expand(rays_o.shape[0], N_SAMPLES)\n\n    pts = rays_o[:, None] + rays_d[:, None] * z_vals[..., None]\n    viewdirs = rays_d / torch.norm(rays_d, dim=-1, keepdim=True)\n    viewdirs = viewdirs[:, None].expand_as(pts)\n\n    pts_enc = posenc(pts.reshape(-1, 3), L_POS)\n    view_enc = posenc(viewdirs.reshape(-1, 3), L_DIR)\n\n    rgb, sigma = model(torch.cat([pts_enc, view_enc], -1))\n    rgb = rgb.reshape(-1, N_SAMPLES, 3)\n    sigma = sigma.reshape(-1, N_SAMPLES)\n\n    dists = z_vals[:, 1:] - z_vals[:, :-1]\n    dists = torch.cat([dists, 1e10 * torch.ones_like(dists[:, :1])], -1)\n\n    alpha = 1.0 - torch.exp(-sigma * dists)\n    T = torch.cumprod(\n        torch.cat([torch.ones_like(alpha[:, :1]), 1 - alpha + 1e-10], -1),\n        -1\n    )[:, :-1]\n    weights = alpha * T\n\n    rgb_map = (weights[..., None] * rgb).sum(dim=1)\n\n    if BACKGROUND == \"white\":\n        rgb_map = rgb_map + (1 - weights.sum(dim=1, keepdim=True))\n\n    return rgb_map\n\n\n\n@torch.no_grad()\ndef render_image(model, rays_o, rays_d):\n    H, W = rays_o.shape[:2]\n    rays_o = rays_o.reshape(-1, 3)\n    rays_d = rays_d.reshape(-1, 3)\n\n    out = []\n    for i in range(0, rays_o.shape[0], CHUNK):\n        rgb = render_rays(\n            model,\n            rays_o[i:i+CHUNK],\n            rays_d[i:i+CHUNK]\n        )\n        out.append(rgb.cpu())\n\n    return torch.cat(out, 0).reshape(H, W, 3)\n\n\n\ndef pose_spherical(theta_deg, radius, device):\n    theta = np.deg2rad(theta_deg)\n\n    cam_pos = torch.tensor(\n        [radius * np.sin(theta), 0.0, radius * np.cos(theta)],\n        device=device,\n        dtype=torch.float32\n    )\n\n    target = torch.tensor([0.0, 0.0, 0.0], device=device, dtype=torch.float32)\n    up = torch.tensor([0.0, 1.0, 0.0], device=device, dtype=torch.float32)\n\n    forward = target - cam_pos\n    forward = forward / torch.norm(forward)\n\n    right = torch.cross(forward, up)\n    right = right / torch.norm(right)\n\n    up = torch.cross(right, forward)\n\n    c2w = torch.eye(4, device=device, dtype=torch.float32)\n    c2w[:3, 0] = right\n    c2w[:3, 1] = up\n    c2w[:3, 2] = -forward\n    c2w[:3, 3] = cam_pos\n\n    return c2w\n   \n\n\ndef main():\n    with open(TRANSFORMS_PATH) as f:\n        meta = json.load(f)\n\n    focal = 0.5 * W / np.tan(0.5 * meta[\"camera_angle_x\"])\n\n    print(f\"Loading checkpoint from {MODEL_PATH}...\")\n    ckpt = torch.load(MODEL_PATH, map_location=DEVICE)\n    \n    model = NeRF().to(DEVICE)\n    print(sum(p.numel() for p in model.parameters()) / 1e6, \"M parameters\")\n    \n    if isinstance(ckpt, dict) and \"model_f\" in ckpt:\n        print(\"Detected full checkpoint → loading fine model\")\n        model.load_state_dict(ckpt[\"model_f\"])\n    elif isinstance(ckpt, dict) and \"state_dict\" in ckpt:\n        model.load_state_dict(ckpt[\"state_dict\"])\n    else:\n        print(\"Detected raw state_dict → loading directly\")\n        model.load_state_dict(ckpt)\n    \n    model.eval()\n\n    print(\"Rendering orbit...\")\n    frames = []  \n    for th in tqdm(np.linspace(0, 360, N_FRAMES, endpoint=False)):\n        c2w = pose_spherical(th, radius=4.0,device = \"cuda\")\n        rays_o, rays_d = get_rays(H, W, focal, c2w)\n        img = render_image(model, rays_o, rays_d)\n        frames.append((img.numpy().clip(0, 1) * 255).astype(np.uint8))\n\n    imageio.mimwrite(OUTPUT_VIDEO, frames, fps=12)\n    print(f\"Saved {OUTPUT_VIDEO}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T09:51:43.849824Z","iopub.execute_input":"2026-01-05T09:51:43.850494Z","iopub.status.idle":"2026-01-05T09:52:54.552676Z","shell.execute_reply.started":"2026-01-05T09:51:43.850464Z","shell.execute_reply":"2026-01-05T09:52:54.551806Z"}},"outputs":[{"name":"stdout","text":"Loading checkpoint from /kaggle/input/model-3k/pytorch/default/1/ckpt_03000.pth...\n0.55066 M parameters\nDetected full checkpoint → loading fine model\nRendering orbit...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 60/60 [01:10<00:00,  1.17s/it]\nIMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (200, 200) to (208, 208) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","output_type":"stream"},{"name":"stdout","text":"Saved orbit.mp4\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}