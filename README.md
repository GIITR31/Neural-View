# Neural-View  
**BYOP 2025 â€“ DSG IIT Roorkee**

Neural-View is a from-scratch implementation of **Neural Radiance Fields (NeRF)** built as part of my BYOP project.  
The aim of this project is to understand NeRF end-to-end by implementing **every stage manually** â€” from preprocessing and camera pose estimation to training, novel view synthesis, orbit video generation, and mesh extraction.


---

## What this project does

Given **only a set of some overlapping images of an object captured from multiple viewpoints**, Neural-View can:

- Estimate camera poses using **COLMAP**
- Convert COLMAP outputs to NeRF format
- Train a **Full NeRF** or **Tiny-NeRF**
- Render unseen viewpoints
- Generate a **360Â° orbit video**
- Extract a **3D mesh (.ply)** using marching cubes

---
## Repository Structure
```text
Neural-View/
â”œâ”€â”€ FULL_NERF/
â”‚   â””â”€â”€ Full NeRF implementation (coarse + fine networks)
â”‚
â”œâ”€â”€ TINY_NERF/
â”‚   â””â”€â”€ Tiny-NeRF implementation (single MLP, faster training)
â”‚
â”œâ”€â”€ preprocessing/
â”‚   â””â”€â”€ COLMAP pipeline and scripts to generate transforms.json
â”‚
â”œâ”€â”€ Statue Test Data/
â”‚   â””â”€â”€ Real-world image dataset (images only, no transforms.json)
â”‚
â”œâ”€â”€ Test Videos/
â”‚   â””â”€â”€ Orbit videos generated by trained NeRF models
â”‚
â”œâ”€â”€ lego_testdata.zip
â”‚   â””â”€â”€ Synthetic NeRF-style dataset (images only)
â”‚
â”œâ”€â”€ EndTermReport.pdf
â”‚   â””â”€â”€ Final project report
â”‚
â””â”€â”€ README.md
    â””â”€â”€ Project documentation and usage guide
```

## Step-by-Step Usage Guide

This section explains how to use **Neural-View** end-to-end â€” from raw images to final 3D outputs.

---

## Step 1: Choose a Dataset

You have **two options**:

### Option A: Use provided synthetic test data (recommended first)
- Use `lego_testdata.zip`
- Contains clean, NeRF-style images
- No background noise
- Best for quick testing and demos

### Option B: Use your own real-world images
- Place your images in a folder (e.g. `my_scene/`)
- Images should:
  - Have good overlap
  - Be taken from nearby viewpoints
  - Cover the object from multiple angles
- These images **do not contain camera poses** and will require COLMAP

---

## Step 1: Camera Pose Estimation (COLMAP)

ğŸ“ **Folder:** `preprocessing/`
-Here first run the `name_change.py` (if your images are not in ordered naming) **Make sure to change the path accordingly** then run the `runcolmap.py` to get the sparse reconstruction **Open the points.ply** to verify the camera positions.
### Output of `runcolmap.py`:
<img width="500" height="400" alt="image" src="https://github.com/user-attachments/assets/ff34d35e-328b-44b3-bd19-9af60d07602e" />

-Then run the `gentransform.py` to generate the `transform.json file` ***Make sure to check the image path in the transform.json it should be renamedimages\img_XX.png**
```text
{
  "file_path": "train/img_035.png",
  "transform_matrix": [
    [ 0.3973,  0.4371, -0.8068,  3.5917 ],
    [ -0.4522,  0.8583,  0.2423,  0.3513 ],
    [ 0.7984,  0.2685,  0.5388, -1.0328 ],
    [ 0.0,     0.0,     0.0,     1.0 ]
  ]
}
```

This step bridges **raw images â†’ NeRF-ready data**.

---

## Step 2: Choose NeRF Variant

You can train **either**:

**Full NeRF** (`FULL_NERF/`)
- Coarse + fine networks
- Hierarchical sampling
- Higher quality reconstruction
- Slower training and higher GPU usage

**Tiny-NeRF** (`TINY_NERF/`)
- Single MLP
- Positional encoding only on *(x, y, z)*
- Much faster training
- Slightly lower quality but very stable

**Recommendation:**
- Start with **Tiny-NeRF**
- Use **Full NeRF** if time and compute allow

---

### Step 3: Training the NeRF Model
This project is designed to be trained **on Kaggle**, since local GPUs (or Mac MPS) are often not sufficient for stable NeRF training.

---

### Setup Training Environment (Kaggle)

1. Upload the project folder (`FULL_NERF/` or `TINY_NERF/`) to Kaggle.
2. Open the **training notebook** inside the folder.
3. Go to **Notebook Settings â†’ Accelerator**  
4. Select **GPU** and choose **2Ã— T4 GPUs**  
 <img width="476" height="128" alt="Screenshot 2026-01-05 at 10 45 28â€¯PM" src="https://github.com/user-attachments/assets/6f42aa10-1c1e-4d0c-951f-edd0c47d6c09" />


This setup allows:
- Larger batch sizes
- Higher image resolution
- Faster convergence

---

### Training Notebook Overview

Each NeRF variant contains **one main training notebook** that:
- Loads images and `transforms_train.json`
- Trains the model
- Saves checkpoints 
- Generates validation images **Make sure to check these to see if the model is learning as the model trains** and PSNR logs

---

## Key Hyperparameters You Can Tune

These hyperparameters are defined near the top of the training notebook.

```python
H_RES, W_RES = 800, 800          # Training image resolution (higher = better detail, more GPU memory)
L_POS, L_DIR = 10, 4             # Positional encoding levels for position and view direction
USE_WHITE_BG = True              # Use True for dark or non-white objects; set False for white or very bright objects

N_ITERS = 50000                  # Total number of training iterations
BATCH_SIZE = 2048                # Number of rays sampled per iteration (higher = smoother training, more VRAM)
LR = 5e-4                        # Learning rate for Adam optimizer

N_COARSE = 64                    # Number of coarse samples per ray (controls initial geometry)
N_FINE = 128                     # Number of fine samples per ray (improves detail via hierarchical sampling)

VAL_INTERVAL = 200               # Run validation and compute PSNR every N iterations
SAVE_INTERVAL = 1000             # Save model checkpoint every N iterations
NOISE_DECAY_ITERS = 2000         # Iterations over which density noise is gradually reduced
```
### Some of the novel views generated by the model (examples)

<div align="center">
  <img src="https://github.com/user-attachments/assets/a113ad48-7ea5-4782-a59a-6f62e200193f" width="600"/>
  <p><em>Just 3000 iters of the full nerf model</em></p>
</div>

<br>

<div align="center">
  <img src="https://github.com/user-attachments/assets/532cc6a1-b0b9-4b62-9a16-6057195356f0" width="600"/>
  <p><em>50k iters of a very cluttered real shoe images</em></p>
</div>

<div align="center">
  <img src="https://github.com/user-attachments/assets/8b855925-f53b-4d31-a3da-3b672b844763" width="600"/>
  <p><em>Drums at 8k iters</em></p>
</div>


## Step 4: 360Â° Orbit Video Generation

Using the trained model:
- A virtual camera is moved in a circular path around the object
- Frames are rendered at each angle
- Frames are combined into a video

ğŸ“ **Output:** `Test Videos/`

This provides an intuitive visualization of the reconstructed scene.

---

## Step 5: 3D Mesh Extraction (Marching Cubes)

To extract geometry:
- A dense 3D grid of points is sampled
- The NeRFâ€™s **density (sigma)** is queried
- A Gaussian filter smooths the density field
- **Marching Cubes** extracts a surface mesh

### Output:
<img width="400" height="400" alt="Screenshot 2026-01-05 at 10 55 36â€¯PM" src="https://github.com/user-attachments/assets/b8e6aa3c-51d5-4081-8d5f-11ba1a72b58a" />

- `.ply` mesh file
- Can be opened in **Blender**, **MeshLab**, etc.

This step converts NeRFâ€™s **implicit representation** into an **explicit 3D mesh**.

---

## Notes & Tips

- Tiny-NeRF is best for quick experimentation
- Full NeRF gives better quality but needs more compute
- Real-world datasets are sensitive to:
  - Camera pose accuracy
  - Background clutter
  - Lighting consistency
- Mesh quality depends heavily on:
  - Density threshold
  - Grid resolution
  - Smoothing strength

---
